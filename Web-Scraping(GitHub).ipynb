{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56465cab",
   "metadata": {},
   "source": [
    "# Web-Scrapping(GitHub)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb15665",
   "metadata": {},
   "source": [
    "Importing the supplimentary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55897bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619fe50b",
   "metadata": {},
   "source": [
    "Create a class for scrapping the github website to find the GitHub Topics and list of repositories related to particular topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef07807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scrap_GitHub:\n",
    "\n",
    "        \n",
    "    def get_topic_title(self, doc):\n",
    "        '''\n",
    "        returns list all Topic Titles from the document page of Github\n",
    "        \n",
    "        '''\n",
    "        self.doc = doc\n",
    "        \n",
    "        # each topic is inspected and topic is collected from the page\n",
    "        class_selections = 'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "        topic_title_tags = self.doc.find_all('p', {'class': class_selections})\n",
    "        topic_link_tags = self.doc.find_all('a', {'class' : 'no-underline flex-grow-0'})\n",
    "        # to get list of all the topic titles\n",
    "        topic_titles = [] \n",
    "        for tag in topic_title_tags:\n",
    "            topic_titles.append(tag.text)\n",
    "        return topic_titles\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_desc(self, doc):\n",
    "        '''\n",
    "        returns list the description of each topic\n",
    "        \n",
    "        '''\n",
    "        self.doc = doc\n",
    "        # getting the description of each topic\n",
    "        description_selection = 'f5 color-fg-muted mb-0 mt-1'\n",
    "        description_tags =  self.doc.find_all('p', {'class': description_selection})\n",
    "        topic_desc =[]\n",
    "        for tag in description_tags:\n",
    "            topic_desc.append(tag.text.strip())\n",
    "        \n",
    "        return topic_desc\n",
    "    \n",
    "    \n",
    "    def get_urls(self, doc):\n",
    "        self.doc = doc\n",
    "        topic_link_tags = doc.find_all('a', {'class' : 'no-underline flex-grow-0'})\n",
    "        topic_urls = []\n",
    "        base_url = 'https://github.com'\n",
    "        for tag in topic_link_tags:\n",
    "            topic_urls.append((base_url + tag['href']).strip())\n",
    "        return topic_urls\n",
    "    \n",
    "    \n",
    "    \n",
    "    def topic(self):\n",
    "        '''\n",
    "        \n",
    "        Returns Topics from the GitHub page in the form of DataFrame.\n",
    "        BeautifulSoup() is used to parse through the data of topic page on GitHub\n",
    "        \n",
    "        '''\n",
    "        topics_url = 'https://github.com/topics'      # Github topic page link\n",
    "        response = requests.get(topics_url)           # gets data from GitHub page\n",
    "        if  response.status_code != 200:              # shows exception\n",
    "            raise Exception(f'Failed to load the page{topics_url}')\n",
    "        page_content = response.text                  # gets text of data\n",
    "        doc = BeautifulSoup(page_content, 'html.parser')   # to parse through the data\n",
    "        \n",
    "        \n",
    "        # creating a dictionary containing information of topic and URL of each Topic\n",
    "        # converting it into a DataFrame \n",
    "        \n",
    "        topic_df = pd.DataFrame({\n",
    "                        'Topics': self.get_topic_title(doc),    # returns topic title\n",
    "                        'Description' : self.get_desc(doc),     # returns Description of topic\n",
    "                        'URLs' : self.get_urls(doc)             # returns URL of each topic\n",
    "        }, index = None)\n",
    "        \n",
    "        return topic_df\n",
    "    def parse_star_count(self, stars_str):\n",
    "        self.stars_str = stars_str\n",
    "        stars_str = self.stars_str.text.strip(' \\n\\n\\n          Star\\n ')\n",
    "        if stars_str[-1] == 'k':\n",
    "            return int(float(stars_str[:-1])*1000) # k means thousand\n",
    "        return int(stars_str)\n",
    "    def select_topic_x(self, topic_page_url):\n",
    "        \"\"\"\n",
    "        returns the list of repositories of selected topic\n",
    "        \"\"\"\n",
    "        self.topic_page_url = topic_page_url\n",
    "        response = requests.get(self.topic_page_url)\n",
    "        topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        return topic_doc\n",
    "    \n",
    "    \n",
    "    def select_topic(self, topic_page_url):\n",
    "        \"\"\"\n",
    "        returns the list of repositories of selected topic\n",
    "        \"\"\"\n",
    "        self.topic_page_url = topic_page_url\n",
    "        \n",
    "        response = requests.get(self.topic_page_url)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception('Failed to load page {}'.format(self.topic_page_url))\n",
    "\n",
    "        topic_doc = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        return topic_doc\n",
    "\n",
    "\n",
    "    def get_repo_info(self, repo_tag, star_tags):\n",
    "        '''\n",
    "        returns all the required info about a repository\n",
    "\n",
    "        '''\n",
    "        self.repo_tag = repo_tag\n",
    "        self.star_tags = star_tags\n",
    "        base_url = 'https://github.com' \n",
    "\n",
    "        a_tags = self.repo_tag.find_all('a')\n",
    "        username = a_tags[0].text.strip()\n",
    "        repo_name = a_tags[1].text.strip()\n",
    "        repo_url = base_url + a_tags[1]['href']\n",
    "        stars = self.parse_star_count(self.star_tags)\n",
    "        return username, repo_name, stars, repo_url\n",
    "    \n",
    "    \n",
    "    def repo_info(self, topic_doc):\n",
    "        self.topic_doc = topic_doc\n",
    "        repo_tags = self.topic_doc.find_all('h3', {'class' : 'f3 color-fg-muted text-normal lh-condensed'})\n",
    "        star_tags = self.topic_doc.find_all('a', {'class' : 'tooltipped tooltipped-s btn-sm btn BtnGroup-item color-bg-default'})\n",
    "\n",
    "        dic = {\n",
    "        'UserName': [],\n",
    "        'Repository Name':[],\n",
    "        'Stars':[],\n",
    "        'URL':[]\n",
    "        }\n",
    "\n",
    "\n",
    "        for i in range(len(repo_tags)):\n",
    "            repo_info = self.get_repo_info(repo_tags[i], star_tags[i])\n",
    "            dic['UserName'].append(repo_info[0])\n",
    "            dic['Repository Name'].append(repo_info[1])\n",
    "            dic['Stars'].append(repo_info[2])\n",
    "            dic['URL'].append(repo_info[3])\n",
    "        return pd.DataFrame(dic)\n",
    "\n",
    "    \n",
    "    \n",
    "    def supplimentary(self, topic_url, path):\n",
    "        \n",
    "        self.topic_url = topic_url\n",
    "        self.path = path\n",
    "        path = self.path + '.csv'\n",
    "        if os.path.exists(path):\n",
    "            print(f'The file {path} already exist. skipping...')\n",
    "            return\n",
    "        topic_df = self.repo_info(self.select_topic(self.topic_url))\n",
    "        topic_df.to_csv(path, index= None)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Main method in this Class\n",
    "    def scrape_all(self):\n",
    "        '''\n",
    "        \n",
    "        This is the Main method of scrape_GitHub class which has accesss to all other methods of the class.\n",
    "        When we call scrape_all using object it will download the data in the form of csv file in the selected folder\n",
    "        Here the folder is GitHub_topics.\n",
    "        \n",
    "        topic() returns the dataframe containting the Topics and their URLs \n",
    "        by using iterrows() we try to iterate over the dataframe of topic and collect information of each topic\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        print(\"Scraping top topics From GitHub\")\n",
    "        topics_df = self.topic()        # returns a DataFrame of topics\n",
    "        os.makedirs('GitHub_Topics', exist_ok = True)      # creates a folder\n",
    "        for index, row in topics_df.iterrows():                     # iterates over a DataFrame and collect info using URLs\n",
    "            print('Scraping top repositories for \"{}\"'.format(row['Topics']))\n",
    "            self.supplimentary(row['URLs'], 'GitHub_Topics/{}'.format(row['Topics']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5390dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Scrap_GitHub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ab2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.scrape_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6968d94",
   "metadata": {},
   "source": [
    "Here is how we can scrape the Github Website.☑️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e2715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
